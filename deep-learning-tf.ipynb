{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Notebook with numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any libraries are added here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test_orig, Y_test_orig) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(1, 12665)"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "Y_tran_binary.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat = X_tran_binary.reshape(X_tran_binary.shape[0], X_tran_binary.shape[1]*X_tran_binary.shape[2]).T\n",
    "\n",
    "X_test_flat = X_test_binary.reshape(X_test_binary.shape[0], X_test_binary.shape[1]*X_test_binary.shape[2]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flat = X_train_flat/255\n",
    "X_test_flat = X_test_flat/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One hot conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    Y = np.eye(C)[Y.reshape(-1)].T\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = convert_to_one_hot(Y_train_orig, 10)\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    mini_batch_size - size of the mini-batches, integer\n",
    "    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters, layers):\n",
    "\n",
    "    \n",
    "    \n",
    "    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n",
    "    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n",
    "    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n",
    "    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n",
    "    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n",
    "    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n",
    "    \n",
    "    params = {\"W1\": W1,\n",
    "              \"b1\": b1,\n",
    "              \"W2\": W2,\n",
    "              \"b2\": b2,\n",
    "              \"W3\": W3,\n",
    "              \"b3\": b3}\n",
    "    \n",
    "    x = tf.placeholder(\"float\", [12288, 1])\n",
    "    \n",
    "    z3 = forward_propagation(x, params, layers)\n",
    "    p = tf.argmax(z3)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    prediction = sess.run(p, feed_dict = {x: X})\n",
    "        \n",
    "    return prediction\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    \"\"\"\n",
    "    Creates the placeholders for the tensorflow session.\n",
    "    \n",
    "    Arguments:\n",
    "    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n",
    "    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n",
    "    \n",
    "    Returns:\n",
    "    X -- placeholder for the data input, of shape [n_x, None] and dtype \"tf.float32\"\n",
    "    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"tf.float32\"\n",
    "    \n",
    "    Tips:\n",
    "    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n",
    "      In fact, the number of examples during test/train is different.\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.placeholder(tf.float32, shape=[n_x, None], name=\"X\")\n",
    "    Y = tf.placeholder(tf.float32, shape=[n_y, None], name=\"Y\")\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layers):\n",
    "    \"\"\"\n",
    "    layers - list of tupples - layer size and activation function\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary of tensors containing weights and biases\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n",
    "    \n",
    "    parameters = {}\n",
    "\n",
    "    for i in len(layers)-1:\n",
    "        parameters['W' + str(i)] = tf.get_variable('W' + str(i), [layers[i+1 ,1], layers[i, 1]], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "        parameters['b' + str(i)] = tf.get_variable('b' + str(i), [layers[i+1, 1], 1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, layers):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input dataset placeholder, of shape (input size, number of examples)\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n",
    "                  the shapes are given in initialize_parameters\n",
    "\n",
    "    Returns:\n",
    "    Z3 -- the output of the last LINEAR unit\n",
    "    \"\"\"\n",
    "    \n",
    "    forward = {}\n",
    "    forward['A0'] = X\n",
    "\n",
    "\n",
    "    for i+1 in len(layers)-2:\n",
    "        forward['Z' + str(i)] = tf.add(tf.matmul(parameters['W' + str(i)], forward['A' + str(i-1)]), parameters['b' + str(i)])\n",
    "        if layers[i,0] == \"relu\":\n",
    "            forward['A' + str(i)] = tf.nn.relu(forward['Z' + str(i)])\n",
    "\n",
    "    forward['Z' + str(len(layers)-1)] = tf.add(tf.matmul(parameters['W' + str(len(layers)-1)], forward['A' + str(len(layers)-2)]), parameters['b' + str(len(layers)-1)])\n",
    "\n",
    "    \n",
    "    # # Retrieve the parameters from the dictionary \"parameters\" \n",
    "    # W1 = parameters['W1']\n",
    "    # b1 = parameters['b1']\n",
    "    # W2 = parameters['W2']\n",
    "    # b2 = parameters['b2']\n",
    "    # W3 = parameters['W3']\n",
    "    # b3 = parameters['b3']\n",
    "    \n",
    "    # ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n",
    "    # Z1 = tf.add(tf.matmul(W1, X), b1)                                              # Z1 = np.dot(W1, X) + b1\n",
    "    # A1 = tf.nn.relu(Z1)                                              # A1 = relu(Z1)\n",
    "    # Z2 = tf.add(tf.matmul(W2, A1), b2)                                              # Z2 = np.dot(W2, A1) + b2\n",
    "    # A2 = tf.nn.relu(Z2)                                              # A2 = relu(Z2)\n",
    "    # Z3 = tf.add(tf.matmul(W3, A2), b3)                                               # Z3 = np.dot(W3, A2) + b3\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return forward['Z' + str(len(layers)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ones(shape):\n",
    "    \"\"\"\n",
    "    Creates an array of ones of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    shape -- shape of the array you want to create\n",
    "        \n",
    "    Returns: \n",
    "    ones -- array containing only ones\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create \"ones\" tensor using tf.ones(...).\n",
    "    ones = tf.ones(shape)\n",
    "    \n",
    "    # Create the session\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Run the session to compute 'ones'\n",
    "    ones = sess.run(ones)\n",
    "    \n",
    "    # Close the session. See method 1 above.\n",
    "    sess.close\n",
    "    \n",
    "    return ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(ZL, Y):\n",
    "    \"\"\"\n",
    "    Computes the cost\n",
    "    \n",
    "    Arguments:\n",
    "    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n",
    "    Y -- \"true\" labels vector placeholder, same shape as Z3\n",
    "    \n",
    "    Returns:\n",
    "    cost - Tensor of the cost function\n",
    "    \"\"\"\n",
    "    \n",
    "    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    logits = tf.transpose(ZL)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n",
    "    \"\"\"\n",
    "    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n",
    "    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n",
    "    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n",
    "    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n",
    "    learning_rate -- learning rate of the optimization\n",
    "    num_epochs -- number of epochs of the optimization loop\n",
    "    minibatch_size -- size of a minibatch\n",
    "    print_cost -- True to print the cost every 100 epochs\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep consistent results\n",
    "    seed = 3                                          # to keep consistent results\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "\n",
    "    X, Y = create_placeholders(n_x, n_y)\n",
    "\n",
    "    parameters = initialize_parameters()\n",
    "\n",
    "    ZL = forward_propagation(X, parameters)\n",
    "    \n",
    "    cost = compute_cost(ZL, Y)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    # Initialize all the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Start the session to compute the tensorflow graph\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # Run the initialization\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Do the training loop\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "\n",
    "                # Select a minibatch\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                \n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost / minibatch_size\n",
    "\n",
    "            # Print the cost every epoch\n",
    "            if print_cost == True and epoch % 100 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per fives)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        # Calculate the correct predictions\n",
    "        correct_prediction = tf.equal(tf.argmax(ZL), tf.argmax(Y))\n",
    "\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import scipy\n",
    "# from PIL import Image\n",
    "# from scipy import ndimage\n",
    "\n",
    "# ## START CODE HERE ## (PUT YOUR IMAGE NAME) \n",
    "# my_image = \"3.jpg\"\n",
    "# ## END CODE HERE ##\n",
    "\n",
    "# # We preprocess your image to fit your algorithm.\n",
    "# fname = \"images/\" + my_image\n",
    "# image = np.array(ndimage.imread(fname, flatten=False))\n",
    "# image = image/255.\n",
    "# my_image = scipy.misc.imresize(image, size=(64,64)).reshape((1, 64*64*3)).T\n",
    "# my_image_prediction = predict(my_image, parameters)\n",
    "\n",
    "# plt.imshow(image)\n",
    "# print(\"Your algorithm predicts: y = \" + str(np.squeeze(my_image_prediction)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit (conda)",
   "language": "python",
   "name": "python38364bitconda02c65cb277b34a2da3ebcbbf996f5d60"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}